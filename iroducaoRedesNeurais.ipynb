{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMJ5OWpSSfFuFB4G5H9aF7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aliybl8YGRA8",
        "outputId": "46a63096-1823-42fd-8aa1-5749f1f08b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/pybrain/pybrain/archive/0.3.3.zip\n",
            "  Using cached https://github.com/pybrain/pybrain/archive/0.3.3.zip\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from PyBrain==0.3.1) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->PyBrain==0.3.1) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "pip install https://github.com/pybrain/pybrain/archive/0.3.3.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pybrain.structure import FeedForwardNetwork\n",
        "from pybrain.structure import LinearLayer,SigmoidLayer, BiasUnit\n",
        "from pybrain.structure import FullConnection \n",
        "\n",
        "rede = FeedForwardNetwork() # Conecta tudo \n",
        "\n",
        "camadaEntrada = LinearLayer(2) #entradas \n",
        "camadaOculta = SigmoidLayer(3) # Oculta \n",
        "camadaSaida = SigmoidLayer(1) # Saida\n",
        "\n",
        "bias1 = BiasUnit()\n",
        "\n",
        "bias2 = BiasUnit()\n",
        "\n",
        "rede.addModule(camadaEntrada)\n",
        "rede.addModule(camadaOculta)\n",
        "rede.addModule(camadaSaida)\n",
        "rede.addModule(bias1)\n",
        "rede.addModule(bias2)\n",
        "\n",
        "entradaOculta = FullConnection(camadaEntrada, camadaOculta)\n",
        "ocultaSaida = FullConnection(camadaOculta, camadaSaida)\n",
        "biasOculta = FullConnection(bias1, camadaOculta)\n",
        "biasSaida = FullConnection(bias2, camadaSaida)\n",
        "\n",
        "rede.sortModules()\n",
        "\n",
        "print(rede) # Mostra a rede \n",
        "print(entradaOculta.params) # Mostra os pesos da camada oculta \n",
        "print(ocultaSaida.params)\n",
        "print(biasOculta.params)\n",
        "print(biasSaida.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XcpLAvdGl1i",
        "outputId": "e8671516-d8a6-4419-a901-cb9bf52286f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNetwork-76\n",
            "   Modules:\n",
            "    [<BiasUnit 'BiasUnit-73'>, <BiasUnit 'BiasUnit-74'>, <LinearLayer 'LinearLayer-71'>, <SigmoidLayer 'SigmoidLayer-72'>, <SigmoidLayer 'SigmoidLayer-75'>]\n",
            "   Connections:\n",
            "    []\n",
            "\n",
            "[-0.18999925 -0.91154148 -1.0311857  -0.02075265 -0.30226975  0.38313546]\n",
            "[ 0.12163055 -0.12315314 -0.80572474]\n",
            "[-0.05320515 -1.17091655 -0.12060243]\n",
            "[0.03091093]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pybrain fazedno o Xor \n",
        "\n",
        "\n",
        "from pybrain.tools.shortcuts import buildNetwork\n",
        "from pybrain.datasets import SupervisedDataSet \n",
        "from pybrain.supervised.trainers import BackpropTrainer\n",
        "from pybrain.structure.modules import SoftmaxLayer\n",
        "from pybrain.structure.modules import SigmoidLayer\n",
        "\n",
        "\n",
        "\n",
        "# Para tirar o bias basta fazer \n",
        "#rede = buildNetwork(2,3,1, outclass = SoftmaxLayer, hiddenclass = SigmoidLayer, bias = false) \n",
        "print(rede['in'])\n",
        "print(rede['hidden0'])\n",
        "print(rede['out'])\n",
        "print(['bias'])\n",
        "\n",
        "rede = buildNetwork(2,3,1)\n",
        "base = SupervisedDataSet(2,1)\n",
        "base.addSample((0,0), (0, ))\n",
        "base.addSample((0,1),(1, ))\n",
        "base.addSample((1,0), (1, ))\n",
        "\n",
        "base.addSample((1,1), (0, ))\n",
        "\n",
        "treinamento = BackpropTrainer(rede, dataset = base, learningrate=0.01,\n",
        "                              momentum = 0.06)\n",
        "\n",
        "for i in range (1,30000):\n",
        "  erro = treinamento.train()\n",
        "  if i % 1000 == 0:\n",
        "    print(\"Erro : %s\" %erro)\n",
        "\n",
        "\n",
        "print(rede.activate([0,0]))\n",
        "\n",
        "print(rede.activate([1,0]))\n",
        "\n",
        "print(rede.activate([0,1]))\n",
        "\n",
        "print(rede.activate([1,1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk56OEmlHK8e",
        "outputId": "06981112-9690-4415-ed93-1fa0eeccc7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<LinearLayer 'in'>\n",
            "<SigmoidLayer 'hidden0'>\n",
            "<LinearLayer 'out'>\n",
            "['bias']\n",
            "Erro : 0.12811301204809408\n",
            "Erro : 0.1262680690813363\n",
            "Erro : 0.1235779469132902\n",
            "Erro : 0.11792749865987877\n",
            "Erro : 0.10925310708452982\n",
            "Erro : 0.10409878330070163\n",
            "Erro : 0.10025018508525659\n",
            "Erro : 0.09675757446378132\n",
            "Erro : 0.0927605573850413\n",
            "Erro : 0.083593685734419\n",
            "Erro : 0.04483478512834495\n",
            "Erro : 0.006200405906516321\n",
            "Erro : 0.00036693823939269537\n",
            "Erro : 1.6893830508898905e-05\n",
            "Erro : 7.372756749481242e-07\n",
            "Erro : 3.1801723591003955e-08\n",
            "Erro : 1.3687665933227055e-09\n",
            "Erro : 5.902023293432744e-11\n",
            "Erro : 2.5497874085605825e-12\n",
            "Erro : 1.1006072335309133e-13\n",
            "Erro : 4.779385315215815e-15\n",
            "Erro : 2.0781833610162916e-16\n",
            "Erro : 9.050354697186974e-18\n",
            "Erro : 3.963635896758948e-19\n",
            "Erro : 1.7374654433732986e-20\n",
            "Erro : 7.655553418557179e-22\n",
            "Erro : 3.387402615682326e-23\n",
            "Erro : 1.496063727410222e-24\n",
            "Erro : 6.781866014097007e-26\n",
            "[-6.66133815e-16]\n",
            "[1.]\n",
            "[1.]\n",
            "[1.3500312e-13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "el0avHvTioCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.fixes import sklearn\n",
        "# Scikit - learn e Iris dataset \n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn import datasets \n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "entradas = iris.data\n",
        "saidas = iris.target \n",
        "redeNeural = MLPClassifier(verbose = True, \n",
        "                           max_iter =1000,\n",
        "                           tol = 0.00001,\n",
        "                           activation = 'logistic')# loss esta em loss\n",
        "\n",
        "redeNeural.fit(entradas,saidas)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcZOGxyNfRYe",
        "outputId": "23017c7d-cfbe-43f8-b1f5-5eb4c3d45d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.15166667\n",
            "Iteration 2, loss = 1.13449871\n",
            "Iteration 3, loss = 1.12072064\n",
            "Iteration 4, loss = 1.11016530\n",
            "Iteration 5, loss = 1.10248371\n",
            "Iteration 6, loss = 1.09711113\n",
            "Iteration 7, loss = 1.09332985\n",
            "Iteration 8, loss = 1.09041044\n",
            "Iteration 9, loss = 1.08775296\n",
            "Iteration 10, loss = 1.08495627\n",
            "Iteration 11, loss = 1.08181058\n",
            "Iteration 12, loss = 1.07825197\n",
            "Iteration 13, loss = 1.07431172\n",
            "Iteration 14, loss = 1.07007429\n",
            "Iteration 15, loss = 1.06564603\n",
            "Iteration 16, loss = 1.06113324\n",
            "Iteration 17, loss = 1.05662770\n",
            "Iteration 18, loss = 1.05219820\n",
            "Iteration 19, loss = 1.04788709\n",
            "Iteration 20, loss = 1.04371088\n",
            "Iteration 21, loss = 1.03966379\n",
            "Iteration 22, loss = 1.03572326\n",
            "Iteration 23, loss = 1.03185621\n",
            "Iteration 24, loss = 1.02802549\n",
            "Iteration 25, loss = 1.02419550\n",
            "Iteration 26, loss = 1.02033648\n",
            "Iteration 27, loss = 1.01642706\n",
            "Iteration 28, loss = 1.01245509\n",
            "Iteration 29, loss = 1.00841695\n",
            "Iteration 30, loss = 1.00431615\n",
            "Iteration 31, loss = 1.00016154\n",
            "Iteration 32, loss = 0.99596539\n",
            "Iteration 33, loss = 0.99174161\n",
            "Iteration 34, loss = 0.98750370\n",
            "Iteration 35, loss = 0.98326283\n",
            "Iteration 36, loss = 0.97902604\n",
            "Iteration 37, loss = 0.97479499\n",
            "Iteration 38, loss = 0.97056578\n",
            "Iteration 39, loss = 0.96632987\n",
            "Iteration 40, loss = 0.96207607\n",
            "Iteration 41, loss = 0.95779305\n",
            "Iteration 42, loss = 0.95347182\n",
            "Iteration 43, loss = 0.94910747\n",
            "Iteration 44, loss = 0.94469972\n",
            "Iteration 45, loss = 0.94025237\n",
            "Iteration 46, loss = 0.93577173\n",
            "Iteration 47, loss = 0.93126479\n",
            "Iteration 48, loss = 0.92673734\n",
            "Iteration 49, loss = 0.92219287\n",
            "Iteration 50, loss = 0.91763209\n",
            "Iteration 51, loss = 0.91305331\n",
            "Iteration 52, loss = 0.90845341\n",
            "Iteration 53, loss = 0.90382889\n",
            "Iteration 54, loss = 0.89917695\n",
            "Iteration 55, loss = 0.89449608\n",
            "Iteration 56, loss = 0.88978626\n",
            "Iteration 57, loss = 0.88504875\n",
            "Iteration 58, loss = 0.88028567\n",
            "Iteration 59, loss = 0.87549946\n",
            "Iteration 60, loss = 0.87069246\n",
            "Iteration 61, loss = 0.86586672\n",
            "Iteration 62, loss = 0.86102383\n",
            "Iteration 63, loss = 0.85616507\n",
            "Iteration 64, loss = 0.85129148\n",
            "Iteration 65, loss = 0.84640404\n",
            "Iteration 66, loss = 0.84150372\n",
            "Iteration 67, loss = 0.83659160\n",
            "Iteration 68, loss = 0.83166889\n",
            "Iteration 69, loss = 0.82673693\n",
            "Iteration 70, loss = 0.82179722\n",
            "Iteration 71, loss = 0.81685144\n",
            "Iteration 72, loss = 0.81190141\n",
            "Iteration 73, loss = 0.80694909\n",
            "Iteration 74, loss = 0.80199650\n",
            "Iteration 75, loss = 0.79704561\n",
            "Iteration 76, loss = 0.79209835\n",
            "Iteration 77, loss = 0.78715646\n",
            "Iteration 78, loss = 0.78222157\n",
            "Iteration 79, loss = 0.77729521\n",
            "Iteration 80, loss = 0.77237888\n",
            "Iteration 81, loss = 0.76747413\n",
            "Iteration 82, loss = 0.76258259\n",
            "Iteration 83, loss = 0.75770599\n",
            "Iteration 84, loss = 0.75284614\n",
            "Iteration 85, loss = 0.74800486\n",
            "Iteration 86, loss = 0.74318393\n",
            "Iteration 87, loss = 0.73838504\n",
            "Iteration 88, loss = 0.73360978\n",
            "Iteration 89, loss = 0.72885963\n",
            "Iteration 90, loss = 0.72413598\n",
            "Iteration 91, loss = 0.71944018\n",
            "Iteration 92, loss = 0.71477352\n",
            "Iteration 93, loss = 0.71013729\n",
            "Iteration 94, loss = 0.70553272\n",
            "Iteration 95, loss = 0.70096099\n",
            "Iteration 96, loss = 0.69642325\n",
            "Iteration 97, loss = 0.69192054\n",
            "Iteration 98, loss = 0.68745386\n",
            "Iteration 99, loss = 0.68302411\n",
            "Iteration 100, loss = 0.67863213\n",
            "Iteration 101, loss = 0.67427870\n",
            "Iteration 102, loss = 0.66996452\n",
            "Iteration 103, loss = 0.66569023\n",
            "Iteration 104, loss = 0.66145640\n",
            "Iteration 105, loss = 0.65726355\n",
            "Iteration 106, loss = 0.65311212\n",
            "Iteration 107, loss = 0.64900251\n",
            "Iteration 108, loss = 0.64493503\n",
            "Iteration 109, loss = 0.64090996\n",
            "Iteration 110, loss = 0.63692751\n",
            "Iteration 111, loss = 0.63298785\n",
            "Iteration 112, loss = 0.62909106\n",
            "Iteration 113, loss = 0.62523720\n",
            "Iteration 114, loss = 0.62142624\n",
            "Iteration 115, loss = 0.61765813\n",
            "Iteration 116, loss = 0.61393274\n",
            "Iteration 117, loss = 0.61024993\n",
            "Iteration 118, loss = 0.60660951\n",
            "Iteration 119, loss = 0.60301125\n",
            "Iteration 120, loss = 0.59945489\n",
            "Iteration 121, loss = 0.59594014\n",
            "Iteration 122, loss = 0.59246667\n",
            "Iteration 123, loss = 0.58903413\n",
            "Iteration 124, loss = 0.58564211\n",
            "Iteration 125, loss = 0.58229022\n",
            "Iteration 126, loss = 0.57897801\n",
            "Iteration 127, loss = 0.57570501\n",
            "Iteration 128, loss = 0.57247074\n",
            "Iteration 129, loss = 0.56927471\n",
            "Iteration 130, loss = 0.56611641\n",
            "Iteration 131, loss = 0.56299529\n",
            "Iteration 132, loss = 0.55991082\n",
            "Iteration 133, loss = 0.55686245\n",
            "Iteration 134, loss = 0.55384962\n",
            "Iteration 135, loss = 0.55087175\n",
            "Iteration 136, loss = 0.54792828\n",
            "Iteration 137, loss = 0.54501862\n",
            "Iteration 138, loss = 0.54214219\n",
            "Iteration 139, loss = 0.53929841\n",
            "Iteration 140, loss = 0.53648668\n",
            "Iteration 141, loss = 0.53370642\n",
            "Iteration 142, loss = 0.53095704\n",
            "Iteration 143, loss = 0.52823795\n",
            "Iteration 144, loss = 0.52554856\n",
            "Iteration 145, loss = 0.52288830\n",
            "Iteration 146, loss = 0.52025658\n",
            "Iteration 147, loss = 0.51765284\n",
            "Iteration 148, loss = 0.51507648\n",
            "Iteration 149, loss = 0.51252696\n",
            "Iteration 150, loss = 0.51000372\n",
            "Iteration 151, loss = 0.50750619\n",
            "Iteration 152, loss = 0.50503383\n",
            "Iteration 153, loss = 0.50258611\n",
            "Iteration 154, loss = 0.50016248\n",
            "Iteration 155, loss = 0.49776243\n",
            "Iteration 156, loss = 0.49538544\n",
            "Iteration 157, loss = 0.49303099\n",
            "Iteration 158, loss = 0.49069858\n",
            "Iteration 159, loss = 0.48838773\n",
            "Iteration 160, loss = 0.48609794\n",
            "Iteration 161, loss = 0.48382874\n",
            "Iteration 162, loss = 0.48157966\n",
            "Iteration 163, loss = 0.47935024\n",
            "Iteration 164, loss = 0.47714002\n",
            "Iteration 165, loss = 0.47494858\n",
            "Iteration 166, loss = 0.47277547\n",
            "Iteration 167, loss = 0.47062027\n",
            "Iteration 168, loss = 0.46848256\n",
            "Iteration 169, loss = 0.46636194\n",
            "Iteration 170, loss = 0.46425800\n",
            "Iteration 171, loss = 0.46217036\n",
            "Iteration 172, loss = 0.46009863\n",
            "Iteration 173, loss = 0.45804245\n",
            "Iteration 174, loss = 0.45600145\n",
            "Iteration 175, loss = 0.45397527\n",
            "Iteration 176, loss = 0.45196356\n",
            "Iteration 177, loss = 0.44996599\n",
            "Iteration 178, loss = 0.44798222\n",
            "Iteration 179, loss = 0.44601194\n",
            "Iteration 180, loss = 0.44405482\n",
            "Iteration 181, loss = 0.44211056\n",
            "Iteration 182, loss = 0.44017887\n",
            "Iteration 183, loss = 0.43825944\n",
            "Iteration 184, loss = 0.43635200\n",
            "Iteration 185, loss = 0.43445628\n",
            "Iteration 186, loss = 0.43257199\n",
            "Iteration 187, loss = 0.43069889\n",
            "Iteration 188, loss = 0.42883672\n",
            "Iteration 189, loss = 0.42698523\n",
            "Iteration 190, loss = 0.42514418\n",
            "Iteration 191, loss = 0.42331334\n",
            "Iteration 192, loss = 0.42149248\n",
            "Iteration 193, loss = 0.41968139\n",
            "Iteration 194, loss = 0.41787986\n",
            "Iteration 195, loss = 0.41608767\n",
            "Iteration 196, loss = 0.41430463\n",
            "Iteration 197, loss = 0.41253055\n",
            "Iteration 198, loss = 0.41076524\n",
            "Iteration 199, loss = 0.40900852\n",
            "Iteration 200, loss = 0.40726021\n",
            "Iteration 201, loss = 0.40552015\n",
            "Iteration 202, loss = 0.40378818\n",
            "Iteration 203, loss = 0.40206413\n",
            "Iteration 204, loss = 0.40034786\n",
            "Iteration 205, loss = 0.39863921\n",
            "Iteration 206, loss = 0.39693806\n",
            "Iteration 207, loss = 0.39524426\n",
            "Iteration 208, loss = 0.39355769\n",
            "Iteration 209, loss = 0.39187821\n",
            "Iteration 210, loss = 0.39020571\n",
            "Iteration 211, loss = 0.38854008\n",
            "Iteration 212, loss = 0.38688120\n",
            "Iteration 213, loss = 0.38522897\n",
            "Iteration 214, loss = 0.38358328\n",
            "Iteration 215, loss = 0.38194405\n",
            "Iteration 216, loss = 0.38031117\n",
            "Iteration 217, loss = 0.37868456\n",
            "Iteration 218, loss = 0.37706413\n",
            "Iteration 219, loss = 0.37544980\n",
            "Iteration 220, loss = 0.37384151\n",
            "Iteration 221, loss = 0.37223917\n",
            "Iteration 222, loss = 0.37064272\n",
            "Iteration 223, loss = 0.36905208\n",
            "Iteration 224, loss = 0.36746721\n",
            "Iteration 225, loss = 0.36588804\n",
            "Iteration 226, loss = 0.36431452\n",
            "Iteration 227, loss = 0.36274659\n",
            "Iteration 228, loss = 0.36118421\n",
            "Iteration 229, loss = 0.35962733\n",
            "Iteration 230, loss = 0.35807590\n",
            "Iteration 231, loss = 0.35652990\n",
            "Iteration 232, loss = 0.35498928\n",
            "Iteration 233, loss = 0.35345401\n",
            "Iteration 234, loss = 0.35192406\n",
            "Iteration 235, loss = 0.35039939\n",
            "Iteration 236, loss = 0.34887999\n",
            "Iteration 237, loss = 0.34736583\n",
            "Iteration 238, loss = 0.34585687\n",
            "Iteration 239, loss = 0.34435312\n",
            "Iteration 240, loss = 0.34285454\n",
            "Iteration 241, loss = 0.34136113\n",
            "Iteration 242, loss = 0.33987286\n",
            "Iteration 243, loss = 0.33838973\n",
            "Iteration 244, loss = 0.33691173\n",
            "Iteration 245, loss = 0.33543884\n",
            "Iteration 246, loss = 0.33397106\n",
            "Iteration 247, loss = 0.33250840\n",
            "Iteration 248, loss = 0.33105083\n",
            "Iteration 249, loss = 0.32959836\n",
            "Iteration 250, loss = 0.32815099\n",
            "Iteration 251, loss = 0.32670872\n",
            "Iteration 252, loss = 0.32527155\n",
            "Iteration 253, loss = 0.32383949\n",
            "Iteration 254, loss = 0.32241253\n",
            "Iteration 255, loss = 0.32099068\n",
            "Iteration 256, loss = 0.31957395\n",
            "Iteration 257, loss = 0.31816234\n",
            "Iteration 258, loss = 0.31675586\n",
            "Iteration 259, loss = 0.31535452\n",
            "Iteration 260, loss = 0.31395833\n",
            "Iteration 261, loss = 0.31256730\n",
            "Iteration 262, loss = 0.31118143\n",
            "Iteration 263, loss = 0.30980075\n",
            "Iteration 264, loss = 0.30842525\n",
            "Iteration 265, loss = 0.30705495\n",
            "Iteration 266, loss = 0.30568987\n",
            "Iteration 267, loss = 0.30433001\n",
            "Iteration 268, loss = 0.30297539\n",
            "Iteration 269, loss = 0.30162602\n",
            "Iteration 270, loss = 0.30028191\n",
            "Iteration 271, loss = 0.29894309\n",
            "Iteration 272, loss = 0.29760956\n",
            "Iteration 273, loss = 0.29628133\n",
            "Iteration 274, loss = 0.29495842\n",
            "Iteration 275, loss = 0.29364084\n",
            "Iteration 276, loss = 0.29232862\n",
            "Iteration 277, loss = 0.29102175\n",
            "Iteration 278, loss = 0.28972026\n",
            "Iteration 279, loss = 0.28842416\n",
            "Iteration 280, loss = 0.28713346\n",
            "Iteration 281, loss = 0.28584818\n",
            "Iteration 282, loss = 0.28456833\n",
            "Iteration 283, loss = 0.28329392\n",
            "Iteration 284, loss = 0.28202497\n",
            "Iteration 285, loss = 0.28076148\n",
            "Iteration 286, loss = 0.27950348\n",
            "Iteration 287, loss = 0.27825097\n",
            "Iteration 288, loss = 0.27700397\n",
            "Iteration 289, loss = 0.27576248\n",
            "Iteration 290, loss = 0.27452652\n",
            "Iteration 291, loss = 0.27329610\n",
            "Iteration 292, loss = 0.27207123\n",
            "Iteration 293, loss = 0.27085192\n",
            "Iteration 294, loss = 0.26963818\n",
            "Iteration 295, loss = 0.26843002\n",
            "Iteration 296, loss = 0.26722745\n",
            "Iteration 297, loss = 0.26603047\n",
            "Iteration 298, loss = 0.26483909\n",
            "Iteration 299, loss = 0.26365333\n",
            "Iteration 300, loss = 0.26247319\n",
            "Iteration 301, loss = 0.26129867\n",
            "Iteration 302, loss = 0.26012978\n",
            "Iteration 303, loss = 0.25896653\n",
            "Iteration 304, loss = 0.25780892\n",
            "Iteration 305, loss = 0.25665695\n",
            "Iteration 306, loss = 0.25551064\n",
            "Iteration 307, loss = 0.25436998\n",
            "Iteration 308, loss = 0.25323498\n",
            "Iteration 309, loss = 0.25210563\n",
            "Iteration 310, loss = 0.25098195\n",
            "Iteration 311, loss = 0.24986393\n",
            "Iteration 312, loss = 0.24875157\n",
            "Iteration 313, loss = 0.24764488\n",
            "Iteration 314, loss = 0.24654385\n",
            "Iteration 315, loss = 0.24544849\n",
            "Iteration 316, loss = 0.24435878\n",
            "Iteration 317, loss = 0.24327473\n",
            "Iteration 318, loss = 0.24219635\n",
            "Iteration 319, loss = 0.24112361\n",
            "Iteration 320, loss = 0.24005653\n",
            "Iteration 321, loss = 0.23899509\n",
            "Iteration 322, loss = 0.23793930\n",
            "Iteration 323, loss = 0.23688915\n",
            "Iteration 324, loss = 0.23584463\n",
            "Iteration 325, loss = 0.23480573\n",
            "Iteration 326, loss = 0.23377245\n",
            "Iteration 327, loss = 0.23274479\n",
            "Iteration 328, loss = 0.23172273\n",
            "Iteration 329, loss = 0.23070628\n",
            "Iteration 330, loss = 0.22969540\n",
            "Iteration 331, loss = 0.22869012\n",
            "Iteration 332, loss = 0.22769040\n",
            "Iteration 333, loss = 0.22669624\n",
            "Iteration 334, loss = 0.22570763\n",
            "Iteration 335, loss = 0.22472457\n",
            "Iteration 336, loss = 0.22374703\n",
            "Iteration 337, loss = 0.22277501\n",
            "Iteration 338, loss = 0.22180849\n",
            "Iteration 339, loss = 0.22084747\n",
            "Iteration 340, loss = 0.21989193\n",
            "Iteration 341, loss = 0.21894185\n",
            "Iteration 342, loss = 0.21799723\n",
            "Iteration 343, loss = 0.21705804\n",
            "Iteration 344, loss = 0.21612429\n",
            "Iteration 345, loss = 0.21519593\n",
            "Iteration 346, loss = 0.21427298\n",
            "Iteration 347, loss = 0.21335540\n",
            "Iteration 348, loss = 0.21244319\n",
            "Iteration 349, loss = 0.21153632\n",
            "Iteration 350, loss = 0.21063478\n",
            "Iteration 351, loss = 0.20973855\n",
            "Iteration 352, loss = 0.20884762\n",
            "Iteration 353, loss = 0.20796197\n",
            "Iteration 354, loss = 0.20708158\n",
            "Iteration 355, loss = 0.20620643\n",
            "Iteration 356, loss = 0.20533650\n",
            "Iteration 357, loss = 0.20447178\n",
            "Iteration 358, loss = 0.20361224\n",
            "Iteration 359, loss = 0.20275787\n",
            "Iteration 360, loss = 0.20190864\n",
            "Iteration 361, loss = 0.20106454\n",
            "Iteration 362, loss = 0.20022555\n",
            "Iteration 363, loss = 0.19939164\n",
            "Iteration 364, loss = 0.19856280\n",
            "Iteration 365, loss = 0.19773901\n",
            "Iteration 366, loss = 0.19692023\n",
            "Iteration 367, loss = 0.19610646\n",
            "Iteration 368, loss = 0.19529768\n",
            "Iteration 369, loss = 0.19449385\n",
            "Iteration 370, loss = 0.19369496\n",
            "Iteration 371, loss = 0.19290098\n",
            "Iteration 372, loss = 0.19211190\n",
            "Iteration 373, loss = 0.19132769\n",
            "Iteration 374, loss = 0.19054833\n",
            "Iteration 375, loss = 0.18977379\n",
            "Iteration 376, loss = 0.18900406\n",
            "Iteration 377, loss = 0.18823911\n",
            "Iteration 378, loss = 0.18747892\n",
            "Iteration 379, loss = 0.18672346\n",
            "Iteration 380, loss = 0.18597271\n",
            "Iteration 381, loss = 0.18522666\n",
            "Iteration 382, loss = 0.18448526\n",
            "Iteration 383, loss = 0.18374851\n",
            "Iteration 384, loss = 0.18301637\n",
            "Iteration 385, loss = 0.18228883\n",
            "Iteration 386, loss = 0.18156585\n",
            "Iteration 387, loss = 0.18084743\n",
            "Iteration 388, loss = 0.18013352\n",
            "Iteration 389, loss = 0.17942411\n",
            "Iteration 390, loss = 0.17871918\n",
            "Iteration 391, loss = 0.17801869\n",
            "Iteration 392, loss = 0.17732263\n",
            "Iteration 393, loss = 0.17663097\n",
            "Iteration 394, loss = 0.17594369\n",
            "Iteration 395, loss = 0.17526076\n",
            "Iteration 396, loss = 0.17458215\n",
            "Iteration 397, loss = 0.17390785\n",
            "Iteration 398, loss = 0.17323783\n",
            "Iteration 399, loss = 0.17257207\n",
            "Iteration 400, loss = 0.17191053\n",
            "Iteration 401, loss = 0.17125320\n",
            "Iteration 402, loss = 0.17060005\n",
            "Iteration 403, loss = 0.16995106\n",
            "Iteration 404, loss = 0.16930620\n",
            "Iteration 405, loss = 0.16866545\n",
            "Iteration 406, loss = 0.16802879\n",
            "Iteration 407, loss = 0.16739618\n",
            "Iteration 408, loss = 0.16676760\n",
            "Iteration 409, loss = 0.16614304\n",
            "Iteration 410, loss = 0.16552246\n",
            "Iteration 411, loss = 0.16490585\n",
            "Iteration 412, loss = 0.16429317\n",
            "Iteration 413, loss = 0.16368441\n",
            "Iteration 414, loss = 0.16307954\n",
            "Iteration 415, loss = 0.16247853\n",
            "Iteration 416, loss = 0.16188136\n",
            "Iteration 417, loss = 0.16128801\n",
            "Iteration 418, loss = 0.16069845\n",
            "Iteration 419, loss = 0.16011266\n",
            "Iteration 420, loss = 0.15953061\n",
            "Iteration 421, loss = 0.15895229\n",
            "Iteration 422, loss = 0.15837767\n",
            "Iteration 423, loss = 0.15780671\n",
            "Iteration 424, loss = 0.15723941\n",
            "Iteration 425, loss = 0.15667574\n",
            "Iteration 426, loss = 0.15611567\n",
            "Iteration 427, loss = 0.15555918\n",
            "Iteration 428, loss = 0.15500624\n",
            "Iteration 429, loss = 0.15445684\n",
            "Iteration 430, loss = 0.15391095\n",
            "Iteration 431, loss = 0.15336855\n",
            "Iteration 432, loss = 0.15282961\n",
            "Iteration 433, loss = 0.15229411\n",
            "Iteration 434, loss = 0.15176203\n",
            "Iteration 435, loss = 0.15123334\n",
            "Iteration 436, loss = 0.15070803\n",
            "Iteration 437, loss = 0.15018607\n",
            "Iteration 438, loss = 0.14966743\n",
            "Iteration 439, loss = 0.14915211\n",
            "Iteration 440, loss = 0.14864006\n",
            "Iteration 441, loss = 0.14813128\n",
            "Iteration 442, loss = 0.14762573\n",
            "Iteration 443, loss = 0.14712340\n",
            "Iteration 444, loss = 0.14662427\n",
            "Iteration 445, loss = 0.14612831\n",
            "Iteration 446, loss = 0.14563551\n",
            "Iteration 447, loss = 0.14514583\n",
            "Iteration 448, loss = 0.14465926\n",
            "Iteration 449, loss = 0.14417578\n",
            "Iteration 450, loss = 0.14369537\n",
            "Iteration 451, loss = 0.14321800\n",
            "Iteration 452, loss = 0.14274365\n",
            "Iteration 453, loss = 0.14227231\n",
            "Iteration 454, loss = 0.14180395\n",
            "Iteration 455, loss = 0.14133855\n",
            "Iteration 456, loss = 0.14087609\n",
            "Iteration 457, loss = 0.14041655\n",
            "Iteration 458, loss = 0.13995992\n",
            "Iteration 459, loss = 0.13950616\n",
            "Iteration 460, loss = 0.13905526\n",
            "Iteration 461, loss = 0.13860721\n",
            "Iteration 462, loss = 0.13816197\n",
            "Iteration 463, loss = 0.13771953\n",
            "Iteration 464, loss = 0.13727987\n",
            "Iteration 465, loss = 0.13684298\n",
            "Iteration 466, loss = 0.13640882\n",
            "Iteration 467, loss = 0.13597739\n",
            "Iteration 468, loss = 0.13554866\n",
            "Iteration 469, loss = 0.13512261\n",
            "Iteration 470, loss = 0.13469923\n",
            "Iteration 471, loss = 0.13427850\n",
            "Iteration 472, loss = 0.13386039\n",
            "Iteration 473, loss = 0.13344489\n",
            "Iteration 474, loss = 0.13303198\n",
            "Iteration 475, loss = 0.13262164\n",
            "Iteration 476, loss = 0.13221386\n",
            "Iteration 477, loss = 0.13180861\n",
            "Iteration 478, loss = 0.13140587\n",
            "Iteration 479, loss = 0.13100564\n",
            "Iteration 480, loss = 0.13060789\n",
            "Iteration 481, loss = 0.13021260\n",
            "Iteration 482, loss = 0.12981976\n",
            "Iteration 483, loss = 0.12942934\n",
            "Iteration 484, loss = 0.12904134\n",
            "Iteration 485, loss = 0.12865573\n",
            "Iteration 486, loss = 0.12827250\n",
            "Iteration 487, loss = 0.12789162\n",
            "Iteration 488, loss = 0.12751309\n",
            "Iteration 489, loss = 0.12713689\n",
            "Iteration 490, loss = 0.12676299\n",
            "Iteration 491, loss = 0.12639139\n",
            "Iteration 492, loss = 0.12602207\n",
            "Iteration 493, loss = 0.12565500\n",
            "Iteration 494, loss = 0.12529018\n",
            "Iteration 495, loss = 0.12492758\n",
            "Iteration 496, loss = 0.12456720\n",
            "Iteration 497, loss = 0.12420901\n",
            "Iteration 498, loss = 0.12385301\n",
            "Iteration 499, loss = 0.12349916\n",
            "Iteration 500, loss = 0.12314747\n",
            "Iteration 501, loss = 0.12279791\n",
            "Iteration 502, loss = 0.12245047\n",
            "Iteration 503, loss = 0.12210513\n",
            "Iteration 504, loss = 0.12176187\n",
            "Iteration 505, loss = 0.12142069\n",
            "Iteration 506, loss = 0.12108157\n",
            "Iteration 507, loss = 0.12074449\n",
            "Iteration 508, loss = 0.12040944\n",
            "Iteration 509, loss = 0.12007640\n",
            "Iteration 510, loss = 0.11974537\n",
            "Iteration 511, loss = 0.11941631\n",
            "Iteration 512, loss = 0.11908923\n",
            "Iteration 513, loss = 0.11876410\n",
            "Iteration 514, loss = 0.11844092\n",
            "Iteration 515, loss = 0.11811966\n",
            "Iteration 516, loss = 0.11780032\n",
            "Iteration 517, loss = 0.11748288\n",
            "Iteration 518, loss = 0.11716732\n",
            "Iteration 519, loss = 0.11685364\n",
            "Iteration 520, loss = 0.11654182\n",
            "Iteration 521, loss = 0.11623184\n",
            "Iteration 522, loss = 0.11592370\n",
            "Iteration 523, loss = 0.11561738\n",
            "Iteration 524, loss = 0.11531287\n",
            "Iteration 525, loss = 0.11501015\n",
            "Iteration 526, loss = 0.11470921\n",
            "Iteration 527, loss = 0.11441004\n",
            "Iteration 528, loss = 0.11411263\n",
            "Iteration 529, loss = 0.11381696\n",
            "Iteration 530, loss = 0.11352302\n",
            "Iteration 531, loss = 0.11323079\n",
            "Iteration 532, loss = 0.11294028\n",
            "Iteration 533, loss = 0.11265146\n",
            "Iteration 534, loss = 0.11236432\n",
            "Iteration 535, loss = 0.11207885\n",
            "Iteration 536, loss = 0.11179504\n",
            "Iteration 537, loss = 0.11151287\n",
            "Iteration 538, loss = 0.11123234\n",
            "Iteration 539, loss = 0.11095343\n",
            "Iteration 540, loss = 0.11067613\n",
            "Iteration 541, loss = 0.11040044\n",
            "Iteration 542, loss = 0.11012633\n",
            "Iteration 543, loss = 0.10985379\n",
            "Iteration 544, loss = 0.10958283\n",
            "Iteration 545, loss = 0.10931341\n",
            "Iteration 546, loss = 0.10904554\n",
            "Iteration 547, loss = 0.10877921\n",
            "Iteration 548, loss = 0.10851439\n",
            "Iteration 549, loss = 0.10825109\n",
            "Iteration 550, loss = 0.10798928\n",
            "Iteration 551, loss = 0.10772897\n",
            "Iteration 552, loss = 0.10747014\n",
            "Iteration 553, loss = 0.10721277\n",
            "Iteration 554, loss = 0.10695686\n",
            "Iteration 555, loss = 0.10670240\n",
            "Iteration 556, loss = 0.10644938\n",
            "Iteration 557, loss = 0.10619778\n",
            "Iteration 558, loss = 0.10594760\n",
            "Iteration 559, loss = 0.10569883\n",
            "Iteration 560, loss = 0.10545146\n",
            "Iteration 561, loss = 0.10520548\n",
            "Iteration 562, loss = 0.10496087\n",
            "Iteration 563, loss = 0.10471763\n",
            "Iteration 564, loss = 0.10447575\n",
            "Iteration 565, loss = 0.10423522\n",
            "Iteration 566, loss = 0.10399603\n",
            "Iteration 567, loss = 0.10375817\n",
            "Iteration 568, loss = 0.10352163\n",
            "Iteration 569, loss = 0.10328640\n",
            "Iteration 570, loss = 0.10305248\n",
            "Iteration 571, loss = 0.10281985\n",
            "Iteration 572, loss = 0.10258850\n",
            "Iteration 573, loss = 0.10235843\n",
            "Iteration 574, loss = 0.10212962\n",
            "Iteration 575, loss = 0.10190208\n",
            "Iteration 576, loss = 0.10167578\n",
            "Iteration 577, loss = 0.10145072\n",
            "Iteration 578, loss = 0.10122690\n",
            "Iteration 579, loss = 0.10100429\n",
            "Iteration 580, loss = 0.10078291\n",
            "Iteration 581, loss = 0.10056273\n",
            "Iteration 582, loss = 0.10034374\n",
            "Iteration 583, loss = 0.10012595\n",
            "Iteration 584, loss = 0.09990934\n",
            "Iteration 585, loss = 0.09969390\n",
            "Iteration 586, loss = 0.09947963\n",
            "Iteration 587, loss = 0.09926651\n",
            "Iteration 588, loss = 0.09905455\n",
            "Iteration 589, loss = 0.09884373\n",
            "Iteration 590, loss = 0.09863404\n",
            "Iteration 591, loss = 0.09842547\n",
            "Iteration 592, loss = 0.09821803\n",
            "Iteration 593, loss = 0.09801169\n",
            "Iteration 594, loss = 0.09780646\n",
            "Iteration 595, loss = 0.09760232\n",
            "Iteration 596, loss = 0.09739927\n",
            "Iteration 597, loss = 0.09719730\n",
            "Iteration 598, loss = 0.09699641\n",
            "Iteration 599, loss = 0.09679658\n",
            "Iteration 600, loss = 0.09659781\n",
            "Iteration 601, loss = 0.09640009\n",
            "Iteration 602, loss = 0.09620341\n",
            "Iteration 603, loss = 0.09600777\n",
            "Iteration 604, loss = 0.09581317\n",
            "Iteration 605, loss = 0.09561958\n",
            "Iteration 606, loss = 0.09542701\n",
            "Iteration 607, loss = 0.09523546\n",
            "Iteration 608, loss = 0.09504490\n",
            "Iteration 609, loss = 0.09485534\n",
            "Iteration 610, loss = 0.09466677\n",
            "Iteration 611, loss = 0.09447918\n",
            "Iteration 612, loss = 0.09429257\n",
            "Iteration 613, loss = 0.09410693\n",
            "Iteration 614, loss = 0.09392225\n",
            "Iteration 615, loss = 0.09373852\n",
            "Iteration 616, loss = 0.09355575\n",
            "Iteration 617, loss = 0.09337392\n",
            "Iteration 618, loss = 0.09319303\n",
            "Iteration 619, loss = 0.09301307\n",
            "Iteration 620, loss = 0.09283403\n",
            "Iteration 621, loss = 0.09265592\n",
            "Iteration 622, loss = 0.09247871\n",
            "Iteration 623, loss = 0.09230242\n",
            "Iteration 624, loss = 0.09212702\n",
            "Iteration 625, loss = 0.09195252\n",
            "Iteration 626, loss = 0.09177891\n",
            "Iteration 627, loss = 0.09160618\n",
            "Iteration 628, loss = 0.09143432\n",
            "Iteration 629, loss = 0.09126334\n",
            "Iteration 630, loss = 0.09109323\n",
            "Iteration 631, loss = 0.09092397\n",
            "Iteration 632, loss = 0.09075557\n",
            "Iteration 633, loss = 0.09058802\n",
            "Iteration 634, loss = 0.09042131\n",
            "Iteration 635, loss = 0.09025544\n",
            "Iteration 636, loss = 0.09009040\n",
            "Iteration 637, loss = 0.08992618\n",
            "Iteration 638, loss = 0.08976279\n",
            "Iteration 639, loss = 0.08960021\n",
            "Iteration 640, loss = 0.08943845\n",
            "Iteration 641, loss = 0.08927749\n",
            "Iteration 642, loss = 0.08911733\n",
            "Iteration 643, loss = 0.08895797\n",
            "Iteration 644, loss = 0.08879939\n",
            "Iteration 645, loss = 0.08864160\n",
            "Iteration 646, loss = 0.08848459\n",
            "Iteration 647, loss = 0.08832835\n",
            "Iteration 648, loss = 0.08817289\n",
            "Iteration 649, loss = 0.08801819\n",
            "Iteration 650, loss = 0.08786424\n",
            "Iteration 651, loss = 0.08771106\n",
            "Iteration 652, loss = 0.08755862\n",
            "Iteration 653, loss = 0.08740693\n",
            "Iteration 654, loss = 0.08725598\n",
            "Iteration 655, loss = 0.08710576\n",
            "Iteration 656, loss = 0.08695628\n",
            "Iteration 657, loss = 0.08680752\n",
            "Iteration 658, loss = 0.08665948\n",
            "Iteration 659, loss = 0.08651216\n",
            "Iteration 660, loss = 0.08636555\n",
            "Iteration 661, loss = 0.08621966\n",
            "Iteration 662, loss = 0.08607446\n",
            "Iteration 663, loss = 0.08592996\n",
            "Iteration 664, loss = 0.08578616\n",
            "Iteration 665, loss = 0.08564305\n",
            "Iteration 666, loss = 0.08550063\n",
            "Iteration 667, loss = 0.08535888\n",
            "Iteration 668, loss = 0.08521781\n",
            "Iteration 669, loss = 0.08507742\n",
            "Iteration 670, loss = 0.08493770\n",
            "Iteration 671, loss = 0.08479864\n",
            "Iteration 672, loss = 0.08466024\n",
            "Iteration 673, loss = 0.08452249\n",
            "Iteration 674, loss = 0.08438540\n",
            "Iteration 675, loss = 0.08424896\n",
            "Iteration 676, loss = 0.08411316\n",
            "Iteration 677, loss = 0.08397800\n",
            "Iteration 678, loss = 0.08384348\n",
            "Iteration 679, loss = 0.08370959\n",
            "Iteration 680, loss = 0.08357633\n",
            "Iteration 681, loss = 0.08344369\n",
            "Iteration 682, loss = 0.08331167\n",
            "Iteration 683, loss = 0.08318027\n",
            "Iteration 684, loss = 0.08304948\n",
            "Iteration 685, loss = 0.08291931\n",
            "Iteration 686, loss = 0.08278973\n",
            "Iteration 687, loss = 0.08266076\n",
            "Iteration 688, loss = 0.08253239\n",
            "Iteration 689, loss = 0.08240461\n",
            "Iteration 690, loss = 0.08227742\n",
            "Iteration 691, loss = 0.08215082\n",
            "Iteration 692, loss = 0.08202480\n",
            "Iteration 693, loss = 0.08189936\n",
            "Iteration 694, loss = 0.08177450\n",
            "Iteration 695, loss = 0.08165021\n",
            "Iteration 696, loss = 0.08152649\n",
            "Iteration 697, loss = 0.08140333\n",
            "Iteration 698, loss = 0.08128074\n",
            "Iteration 699, loss = 0.08115871\n",
            "Iteration 700, loss = 0.08103723\n",
            "Iteration 701, loss = 0.08091630\n",
            "Iteration 702, loss = 0.08079593\n",
            "Iteration 703, loss = 0.08067610\n",
            "Iteration 704, loss = 0.08055681\n",
            "Iteration 705, loss = 0.08043806\n",
            "Iteration 706, loss = 0.08031984\n",
            "Iteration 707, loss = 0.08020216\n",
            "Iteration 708, loss = 0.08008501\n",
            "Iteration 709, loss = 0.07996839\n",
            "Iteration 710, loss = 0.07985229\n",
            "Iteration 711, loss = 0.07973670\n",
            "Iteration 712, loss = 0.07962164\n",
            "Iteration 713, loss = 0.07950709\n",
            "Iteration 714, loss = 0.07939305\n",
            "Iteration 715, loss = 0.07927951\n",
            "Iteration 716, loss = 0.07916649\n",
            "Iteration 717, loss = 0.07905396\n",
            "Iteration 718, loss = 0.07894193\n",
            "Iteration 719, loss = 0.07883040\n",
            "Iteration 720, loss = 0.07871936\n",
            "Iteration 721, loss = 0.07860881\n",
            "Iteration 722, loss = 0.07849875\n",
            "Iteration 723, loss = 0.07838917\n",
            "Iteration 724, loss = 0.07828008\n",
            "Iteration 725, loss = 0.07817146\n",
            "Iteration 726, loss = 0.07806332\n",
            "Iteration 727, loss = 0.07795565\n",
            "Iteration 728, loss = 0.07784845\n",
            "Iteration 729, loss = 0.07774172\n",
            "Iteration 730, loss = 0.07763545\n",
            "Iteration 731, loss = 0.07752965\n",
            "Iteration 732, loss = 0.07742430\n",
            "Iteration 733, loss = 0.07731941\n",
            "Iteration 734, loss = 0.07721498\n",
            "Iteration 735, loss = 0.07711100\n",
            "Iteration 736, loss = 0.07700746\n",
            "Iteration 737, loss = 0.07690437\n",
            "Iteration 738, loss = 0.07680173\n",
            "Iteration 739, loss = 0.07669953\n",
            "Iteration 740, loss = 0.07659776\n",
            "Iteration 741, loss = 0.07649643\n",
            "Iteration 742, loss = 0.07639554\n",
            "Iteration 743, loss = 0.07629507\n",
            "Iteration 744, loss = 0.07619504\n",
            "Iteration 745, loss = 0.07609543\n",
            "Iteration 746, loss = 0.07599624\n",
            "Iteration 747, loss = 0.07589748\n",
            "Iteration 748, loss = 0.07579913\n",
            "Iteration 749, loss = 0.07570120\n",
            "Iteration 750, loss = 0.07560369\n",
            "Iteration 751, loss = 0.07550658\n",
            "Iteration 752, loss = 0.07540989\n",
            "Iteration 753, loss = 0.07531360\n",
            "Iteration 754, loss = 0.07521772\n",
            "Iteration 755, loss = 0.07512224\n",
            "Iteration 756, loss = 0.07502716\n",
            "Iteration 757, loss = 0.07493248\n",
            "Iteration 758, loss = 0.07483819\n",
            "Iteration 759, loss = 0.07474430\n",
            "Iteration 760, loss = 0.07465080\n",
            "Iteration 761, loss = 0.07455769\n",
            "Iteration 762, loss = 0.07446497\n",
            "Iteration 763, loss = 0.07437263\n",
            "Iteration 764, loss = 0.07428067\n",
            "Iteration 765, loss = 0.07418909\n",
            "Iteration 766, loss = 0.07409789\n",
            "Iteration 767, loss = 0.07400707\n",
            "Iteration 768, loss = 0.07391662\n",
            "Iteration 769, loss = 0.07382655\n",
            "Iteration 770, loss = 0.07373684\n",
            "Iteration 771, loss = 0.07364751\n",
            "Iteration 772, loss = 0.07355853\n",
            "Iteration 773, loss = 0.07346993\n",
            "Iteration 774, loss = 0.07338168\n",
            "Iteration 775, loss = 0.07329380\n",
            "Iteration 776, loss = 0.07320627\n",
            "Iteration 777, loss = 0.07311910\n",
            "Iteration 778, loss = 0.07303228\n",
            "Iteration 779, loss = 0.07294581\n",
            "Iteration 780, loss = 0.07285970\n",
            "Iteration 781, loss = 0.07277393\n",
            "Iteration 782, loss = 0.07268851\n",
            "Iteration 783, loss = 0.07260343\n",
            "Iteration 784, loss = 0.07251870\n",
            "Iteration 785, loss = 0.07243431\n",
            "Iteration 786, loss = 0.07235025\n",
            "Iteration 787, loss = 0.07226654\n",
            "Iteration 788, loss = 0.07218315\n",
            "Iteration 789, loss = 0.07210011\n",
            "Iteration 790, loss = 0.07201739\n",
            "Iteration 791, loss = 0.07193500\n",
            "Iteration 792, loss = 0.07185294\n",
            "Iteration 793, loss = 0.07177121\n",
            "Iteration 794, loss = 0.07168980\n",
            "Iteration 795, loss = 0.07160872\n",
            "Iteration 796, loss = 0.07152795\n",
            "Iteration 797, loss = 0.07144751\n",
            "Iteration 798, loss = 0.07136738\n",
            "Iteration 799, loss = 0.07128757\n",
            "Iteration 800, loss = 0.07120807\n",
            "Iteration 801, loss = 0.07112889\n",
            "Iteration 802, loss = 0.07105002\n",
            "Iteration 803, loss = 0.07097145\n",
            "Iteration 804, loss = 0.07089320\n",
            "Iteration 805, loss = 0.07081525\n",
            "Iteration 806, loss = 0.07073760\n",
            "Iteration 807, loss = 0.07066026\n",
            "Iteration 808, loss = 0.07058321\n",
            "Iteration 809, loss = 0.07050647\n",
            "Iteration 810, loss = 0.07043003\n",
            "Iteration 811, loss = 0.07035388\n",
            "Iteration 812, loss = 0.07027802\n",
            "Iteration 813, loss = 0.07020246\n",
            "Iteration 814, loss = 0.07012720\n",
            "Iteration 815, loss = 0.07005222\n",
            "Iteration 816, loss = 0.06997753\n",
            "Iteration 817, loss = 0.06990313\n",
            "Iteration 818, loss = 0.06982901\n",
            "Iteration 819, loss = 0.06975518\n",
            "Iteration 820, loss = 0.06968163\n",
            "Iteration 821, loss = 0.06960836\n",
            "Iteration 822, loss = 0.06953537\n",
            "Iteration 823, loss = 0.06946266\n",
            "Iteration 824, loss = 0.06939023\n",
            "Iteration 825, loss = 0.06931807\n",
            "Iteration 826, loss = 0.06924619\n",
            "Iteration 827, loss = 0.06917458\n",
            "Iteration 828, loss = 0.06910324\n",
            "Iteration 829, loss = 0.06903217\n",
            "Iteration 830, loss = 0.06896137\n",
            "Iteration 831, loss = 0.06889083\n",
            "Iteration 832, loss = 0.06882056\n",
            "Iteration 833, loss = 0.06875056\n",
            "Iteration 834, loss = 0.06868082\n",
            "Iteration 835, loss = 0.06861134\n",
            "Iteration 836, loss = 0.06854212\n",
            "Iteration 837, loss = 0.06847316\n",
            "Iteration 838, loss = 0.06840446\n",
            "Iteration 839, loss = 0.06833601\n",
            "Iteration 840, loss = 0.06826782\n",
            "Iteration 841, loss = 0.06819988\n",
            "Iteration 842, loss = 0.06813220\n",
            "Iteration 843, loss = 0.06806476\n",
            "Iteration 844, loss = 0.06799758\n",
            "Iteration 845, loss = 0.06793065\n",
            "Iteration 846, loss = 0.06786396\n",
            "Iteration 847, loss = 0.06779752\n",
            "Iteration 848, loss = 0.06773132\n",
            "Iteration 849, loss = 0.06766537\n",
            "Iteration 850, loss = 0.06759966\n",
            "Iteration 851, loss = 0.06753419\n",
            "Iteration 852, loss = 0.06746896\n",
            "Iteration 853, loss = 0.06740397\n",
            "Iteration 854, loss = 0.06733922\n",
            "Iteration 855, loss = 0.06727470\n",
            "Iteration 856, loss = 0.06721042\n",
            "Iteration 857, loss = 0.06714637\n",
            "Iteration 858, loss = 0.06708256\n",
            "Iteration 859, loss = 0.06701898\n",
            "Iteration 860, loss = 0.06695563\n",
            "Iteration 861, loss = 0.06689251\n",
            "Iteration 862, loss = 0.06682961\n",
            "Iteration 863, loss = 0.06676695\n",
            "Iteration 864, loss = 0.06670451\n",
            "Iteration 865, loss = 0.06664229\n",
            "Iteration 866, loss = 0.06658030\n",
            "Iteration 867, loss = 0.06651854\n",
            "Iteration 868, loss = 0.06645699\n",
            "Iteration 869, loss = 0.06639566\n",
            "Iteration 870, loss = 0.06633456\n",
            "Iteration 871, loss = 0.06627367\n",
            "Iteration 872, loss = 0.06621300\n",
            "Iteration 873, loss = 0.06615255\n",
            "Iteration 874, loss = 0.06609231\n",
            "Iteration 875, loss = 0.06603229\n",
            "Iteration 876, loss = 0.06597247\n",
            "Iteration 877, loss = 0.06591288\n",
            "Iteration 878, loss = 0.06585349\n",
            "Iteration 879, loss = 0.06579431\n",
            "Iteration 880, loss = 0.06573534\n",
            "Iteration 881, loss = 0.06567658\n",
            "Iteration 882, loss = 0.06561803\n",
            "Iteration 883, loss = 0.06555968\n",
            "Iteration 884, loss = 0.06550154\n",
            "Iteration 885, loss = 0.06544360\n",
            "Iteration 886, loss = 0.06538587\n",
            "Iteration 887, loss = 0.06532833\n",
            "Iteration 888, loss = 0.06527100\n",
            "Iteration 889, loss = 0.06521387\n",
            "Iteration 890, loss = 0.06515694\n",
            "Iteration 891, loss = 0.06510021\n",
            "Iteration 892, loss = 0.06504367\n",
            "Iteration 893, loss = 0.06498733\n",
            "Iteration 894, loss = 0.06493119\n",
            "Iteration 895, loss = 0.06487524\n",
            "Iteration 896, loss = 0.06481948\n",
            "Iteration 897, loss = 0.06476392\n",
            "Iteration 898, loss = 0.06470855\n",
            "Iteration 899, loss = 0.06465336\n",
            "Iteration 900, loss = 0.06459837\n",
            "Iteration 901, loss = 0.06454357\n",
            "Iteration 902, loss = 0.06448896\n",
            "Iteration 903, loss = 0.06443453\n",
            "Iteration 904, loss = 0.06438029\n",
            "Iteration 905, loss = 0.06432624\n",
            "Iteration 906, loss = 0.06427237\n",
            "Iteration 907, loss = 0.06421868\n",
            "Iteration 908, loss = 0.06416518\n",
            "Iteration 909, loss = 0.06411186\n",
            "Iteration 910, loss = 0.06405872\n",
            "Iteration 911, loss = 0.06400576\n",
            "Iteration 912, loss = 0.06395298\n",
            "Iteration 913, loss = 0.06390038\n",
            "Iteration 914, loss = 0.06384796\n",
            "Iteration 915, loss = 0.06379571\n",
            "Iteration 916, loss = 0.06374364\n",
            "Iteration 917, loss = 0.06369175\n",
            "Iteration 918, loss = 0.06364003\n",
            "Iteration 919, loss = 0.06358849\n",
            "Iteration 920, loss = 0.06353711\n",
            "Iteration 921, loss = 0.06348591\n",
            "Iteration 922, loss = 0.06343488\n",
            "Iteration 923, loss = 0.06338403\n",
            "Iteration 924, loss = 0.06333334\n",
            "Iteration 925, loss = 0.06328282\n",
            "Iteration 926, loss = 0.06323247\n",
            "Iteration 927, loss = 0.06318228\n",
            "Iteration 928, loss = 0.06313227\n",
            "Iteration 929, loss = 0.06308242\n",
            "Iteration 930, loss = 0.06303273\n",
            "Iteration 931, loss = 0.06298321\n",
            "Iteration 932, loss = 0.06293385\n",
            "Iteration 933, loss = 0.06288466\n",
            "Iteration 934, loss = 0.06283562\n",
            "Iteration 935, loss = 0.06278675\n",
            "Iteration 936, loss = 0.06273804\n",
            "Iteration 937, loss = 0.06268949\n",
            "Iteration 938, loss = 0.06264110\n",
            "Iteration 939, loss = 0.06259287\n",
            "Iteration 940, loss = 0.06254480\n",
            "Iteration 941, loss = 0.06249688\n",
            "Iteration 942, loss = 0.06244912\n",
            "Iteration 943, loss = 0.06240152\n",
            "Iteration 944, loss = 0.06235407\n",
            "Iteration 945, loss = 0.06230677\n",
            "Iteration 946, loss = 0.06225963\n",
            "Iteration 947, loss = 0.06221264\n",
            "Iteration 948, loss = 0.06216580\n",
            "Iteration 949, loss = 0.06211912\n",
            "Iteration 950, loss = 0.06207259\n",
            "Iteration 951, loss = 0.06202620\n",
            "Iteration 952, loss = 0.06197997\n",
            "Iteration 953, loss = 0.06193388\n",
            "Iteration 954, loss = 0.06188795\n",
            "Iteration 955, loss = 0.06184216\n",
            "Iteration 956, loss = 0.06179652\n",
            "Iteration 957, loss = 0.06175102\n",
            "Iteration 958, loss = 0.06170567\n",
            "Iteration 959, loss = 0.06166046\n",
            "Iteration 960, loss = 0.06161540\n",
            "Iteration 961, loss = 0.06157049\n",
            "Iteration 962, loss = 0.06152571\n",
            "Iteration 963, loss = 0.06148108\n",
            "Iteration 964, loss = 0.06143659\n",
            "Iteration 965, loss = 0.06139224\n",
            "Iteration 966, loss = 0.06134803\n",
            "Iteration 967, loss = 0.06130397\n",
            "Iteration 968, loss = 0.06126004\n",
            "Iteration 969, loss = 0.06121625\n",
            "Iteration 970, loss = 0.06117260\n",
            "Iteration 971, loss = 0.06112908\n",
            "Iteration 972, loss = 0.06108571\n",
            "Iteration 973, loss = 0.06104246\n",
            "Iteration 974, loss = 0.06099936\n",
            "Iteration 975, loss = 0.06095639\n",
            "Iteration 976, loss = 0.06091355\n",
            "Iteration 977, loss = 0.06087085\n",
            "Iteration 978, loss = 0.06082829\n",
            "Iteration 979, loss = 0.06078585\n",
            "Iteration 980, loss = 0.06074355\n",
            "Iteration 981, loss = 0.06070138\n",
            "Iteration 982, loss = 0.06065934\n",
            "Iteration 983, loss = 0.06061743\n",
            "Iteration 984, loss = 0.06057565\n",
            "Iteration 985, loss = 0.06053400\n",
            "Iteration 986, loss = 0.06049248\n",
            "Iteration 987, loss = 0.06045109\n",
            "Iteration 988, loss = 0.06040982\n",
            "Iteration 989, loss = 0.06036868\n",
            "Iteration 990, loss = 0.06032767\n",
            "Iteration 991, loss = 0.06028679\n",
            "Iteration 992, loss = 0.06024603\n",
            "Iteration 993, loss = 0.06020540\n",
            "Iteration 994, loss = 0.06016489\n",
            "Iteration 995, loss = 0.06012450\n",
            "Iteration 996, loss = 0.06008424\n",
            "Iteration 997, loss = 0.06004410\n",
            "Iteration 998, loss = 0.06000408\n",
            "Iteration 999, loss = 0.05996419\n",
            "Iteration 1000, loss = 0.05992442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "redeNeural.predict([[5,7.2,5.1,2.2]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsVyEoAajr36",
        "outputId": "fe5f7d01-aa8a-4661-c6e5-39a99c14dd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDScAXB_o2A6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}